# LLM Serving in a Week

[Preface](./preface.md)
[Setting Up the Environment](./setup.md)

---

- [Week 1: From Matmul to Text](./week1-overview.md)
    - [Attention and Multi-Head Attention](./week1-01-attention.md)
    - [Positional Encodings and RoPE](./week1-02-positional-encodings.md)
    - [Grouped/Multi Query Attention](./week1-03-gqa.md)
    - [Multilayer Perceptron Layer and Transformer]()
    - [Wiring the Qwen2 Model]()
    - [Loading the Model]()
    - [Generating the Response]()
    <!--
    - [Attention and Multi-Head Attention](./week1-01-attention.md)
    - [Positional Embeddings and RoPE](./week1-02-positional-embeddings.md)
    - [Grouped/Multi Query Attention](./week1-03-gqa.md)
    - [Multilayer Perceptron Layer and Transformer](./week1-04-mlp-transformer.md)
    - [Wiring the Qwen2 Model](./week1-05-model-1.md)
    - [Loading the Model](./week1-06-model-2.md)
    - [Generating the Response](./week1-07-generate.md)
    -->

- [Week 2: Optimizing]()

- [Week 3: Serving]()

---

[Glossary Index](./glossary.md)
